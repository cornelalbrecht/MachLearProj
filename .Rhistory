)
dcomb <- train(diagnosis~., data = data.frame(cbind(pa,pb,pc, testing$diagnosis)), method = "rf")
length(pa)
length(pb)
length(pc)
dcomb <- train(diagnosis~., data = cbind(pa,pb,pc, testing$diagnosis), method = "rf")
dcomb <- train(diagnosis~., data = cbind(pa,pb,pc, diagnosistesting$diagnosis), method = "rf")
dcomb <- train(diagnosis~., data = cbind(pa,pb,pc, diagnosis = testing$diagnosis), method = "rf")
dcomb
epred <- predict(dcomb)
confusionMatrix(epred, testing$diagnosis)
set.seed(62433)
mod_rf <- train(diagnosis ~ ., data = training, method = "rf")
mod_gbm <- train(diagnosis ~ ., data = training, method = "gbm")
mod_lda <- train(diagnosis ~ ., data = training, method = "lda")
pred_rf <- predict(mod_rf, testing)
pred_gbm <- predict(mod_gbm, testing)
pred_lda <- predict(mod_lda, testing)
predDF <- data.frame(pred_rf, pred_gbm, pred_lda, diagnosis = testing$diagnosis)
combModFit <- train(diagnosis ~ ., method = "rf", data = predDF)
combPred <- predict(combModFit, predDF)
library(caret)
library(gbm)
set.seed(3433)
library(AppliedPredictiveModeling)
data(AlzheimerDisease)
adData = data.frame(diagnosis, predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]
training = adData[inTrain, ]
testing = adData[-inTrain, ]
set.seed(62433)
mod_rf <- train(diagnosis ~ ., data = training, method = "rf")
mod_gbm <- train(diagnosis ~ ., data = training, method = "gbm")
mod_lda <- train(diagnosis ~ ., data = training, method = "lda")
pred_rf <- predict(mod_rf, testing)
pred_gbm <- predict(mod_gbm, testing)
pred_lda <- predict(mod_lda, testing)
predDF <- data.frame(pred_rf, pred_gbm, pred_lda, diagnosis = testing$diagnosis)
combModFit <- train(diagnosis ~ ., method = "rf", data = predDF)
combPred <- predict(combModFit, predDF)
confusionMatrix(pred_rf, testing$diagnosis)
confusionMatrix(combPred, testing$diagnosis)
confusionMatrix(pred_lda, testing$diagnosis)
confusionMatrix(pred_gbm, testing$diagnosis)
set.seed(3523)
library(AppliedPredictiveModeling)
data(concrete)
inTrain = createDataPartition(concrete$CompressiveStrength, p = 3/4)[[1]]
training = concrete[inTrain, ]
testing = concrete[-inTrain, ]
set.seed(233)
mod_lasso <- train(CompressiveStrength ~ ., data = training, method = "lasso")
set.seed(233)
mod_lasso <- train(CompressiveStrength ~ ., data = training, method = "lasso")
set.seed(233)
mod_lasso <- train(CompressiveStrength ~ ., data = training, method = "lasso")
library(elasticnet)
plot.enet(mod_lasso$finalModel, xvar = "penalty", use.color = TRUE)
set.seed(3523)
library(AppliedPredictiveModeling)
data(concrete)
inTrain = createDataPartition(concrete$CompressiveStrength, p = 3/4)[[1]]
training = concrete[inTrain, ]
testing = concrete[-inTrain, ]
set.seed(325)
library(e1071)
mod_svm <- svm(CompressiveStrength ~ ., data = training)
pred_svm <- predict(mod_svm, testing)
accuracy(pred_svm, testing$CompressiveStrength)
pred_scv
pred_svm
library(MASS)
head(shuttle)
shuttle2<-shuttle
shuttle2$use2<-as.numeric(shuttle2$use=='auto')
#shuttle2$wind2<-as.numeric(shuttle2$wind=='head')
#head(shuttle2)
fit<-glm(use2 ~ factor(wind) - 1, family = binomial, data = shuttle2)
View(shuttle)
View(shuttle2)
fit
?glm
summary(fit)$coef
exp(coef(fit))
exp(cbind(OddsRatio = coef(fit), confint(fit)))
shuttle$usebin <- as.numeric(shuttle$use == "auto") # create a binary variable
fit <- glm(usebin ~ factor(wind) - 1, family = "binomial", data = shuttle)
Coef <- coef(summary(fit))
coef.odds <- exp(c(Coef[1, 1], Coef[2, 1]))
(odds.ratio <- coef.odds[1] / coef.odds[2]) # "head" is the reference
fit<-glm(use2 ~ factor(wind) + factor(magn) - 1, family = binomial, data = shuttle2)
summary(fit)$coef
fit<-glm(count~factor(spray)-1,data=InsectSprays,family=poisson)
summary(fit)$coef
2.674/2.73
x <- -5:5
y <- c(5.12, 3.93, 2.67, 1.87, 0.52, 0.08, 0.93, 2.05, 2.54, 3.87, 4.97)
knots <- 0
splineTerms <- sapply(knots, function(knot) (x > knot) * (x - knot))
(xMat <- cbind(1, x, splineTerms))
View(xMat)
(fit6 <- lm(y ~ xMat - 1))
q()
setwd("~/Google Drive/DataScienceClasses/Machine Learning")
if(!file.exists("train.csv")){
download.file("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv", "train.csv", method = "curl")
}
if(!file.exists("test.csv")){
download.file("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv", "train.csv", method = "curl")
}
if(!file.exists("train.csv")){
download.file("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv", "train.csv", method = "curl")
}
if(!file.exists("test.csv")){
download.file("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv", "test.csv", method = "curl")
}
train <- read.csv("train.csv")
test <- read.csv("test.csv")
View(test)
train$classe
summary(train)
apply(train,2,sum(is.na()))
apply(train,2,is.na)
apply(apply(train,2,is.na),1,sum)
apply(train,2,max)
apply(train,2,min)
range(train)
is.na(train$total_accel_belt)
summary(train)
train <- read.csv("train.csv", na.strings = c("","NA","#DIV/0!"))
summary(train)
View(train)
train <- read.csv("train.csv")
summary(train)
a <- factor(train$max_yaw_forearm)
levels(a)
train <- read.csv("train.csv", na.strings = c("","NA","#DIV/0!"))
summary(train)
19622-1962
View(test)
test <- read.csv("test.csv", na.strings = c("","NA","#DIV/0!"))
View(test)
summary(test)
is.na(test)
colSums(test, na.rm = T)
View(test)
apply(train, 2, function(x) mean(is.na(x))) > .9)
apply(train, 2, function(x) mean(is.na(x)) > .9)
a <- apply(train, 2, function(x) mean(is.na(x)) > .9)
a
View(data.frame(a))
sum(a)
removeNA <- apply(train, 2, function(x){mean(is.na(x))})
removeNA
hist(removeNA)
summary(factor(removeNA))
levels(factor(removeNA))
removeNA <- apply(train, 2, function(x){mean(is.na(x))})
levels(factor(removeNA))
removeNA <- removeNA > 0.9
str(removeNA)
class(removeNA)
View(test)
removeSkew <- rep(FALSE, 7)
remove <- append(removeSkew, removeNA)
remove
removeNA <- apply(train, 2, function(x){mean(is.na(x))})
#levels(factor(removeNA))
removeNA <- removeNA > 0.9
removeNA
removeNA[1:7] = TRUE
removeNA
remove <- apply(train, 2, function(x){mean(is.na(x))})
#levels(factor(removeNA))
remove <- removeNA > 0.9
# This takes care of the first 7 colums
remove
remove <- apply(train, 2, function(x){mean(is.na(x))})
#levels(factor(removeNA))
remove <- removeNA > 0.9
# This takes care of the first 7 colums
remove
rm(remove)
rm(removeNA)
remove <- apply(train, 2, function(x){mean(is.na(x))})
remove
remove <- removeNA > 0.9
remove <- remove > 0.9
remove
remove
remove[1:7] = TRUE
remove
train <- train[,!remove]
test <- test[,!remove]
View(test)
View(train)
library(dplyr)
library(caret)
library(ggplot2)
insamp <- createDataPartition(train, p = 0.65, list = FALSE)
insamp <- createDataPartition(train$classe, p = 0.65, list = FALSE)
View(insamp)
hist(insamp)
insTrain <- train[insamp,]
outTrain <- train[-isamp,]
insTrain <- train[insamp, ]
outTrain <- train[-insamp, ]
?trainControl
?train
trnCtrl <- trainControl(method = "cv", number = 5)
RFfit <- train(classe ~ ., data = insTrain, trControl = trnCtrl)
trnCtrl <- trainControl(method = "cv", number = 5)
RFfit <- train(classe ~ ., data = insTrain, trControl = trnCtrl, method = "rf")
trnCtrl <- trainControl(method = "cv", number = 1)
RFfit <- train(classe ~ ., data = insTrain, trControl = trnCtrl, method = "rf")
str(insTrain)
GBfit <- train(classe ~ ., data = insTrain, trControl = trnCtrl, method = "gbm")
trnCtrl <- trainControl(method = "cv", number = 2)
GBfit <- train(classe ~ ., data = insTrain, trControl = trnCtrl, method = "gbm")
GBfitr
GBfit
confusionMatrix(outTrain$classe, predict(GBfit, outTrain))
trnCtrl <- trainControl(method = "cv", number = 2)
RFfit <- train(classe ~ ., data = insTrain, trControl = trnCtrl, method = "rf")
confusionMatrix(outTrain$classe, predict(RFfit, outTrain))
varImpPlot(RFfit$finalModel)
library(rpart)
Sys.time(sdsd)
?Sys.time
tmp <- trainControl(type = "none")
RFfit <- train(classe ~ ., data = insTrain, trControl = tmp, method = "rf")
tmp <- trainControl(type = "none")
RFfit <- train(classe ~ ., data = insTrain, trControl = tmp, method = "rf")
varImpPlot(RFfit$finalModel)
confusionMatrix(outTrain$classe, predict(RFfit, outTrain))
tmp <- trainControl(type = "none", verboseIter = T)
# Random Forest
RFfit <- train(classe ~ ., data = insTrain, trControl = tmp, method = "rf")
tmp <- trainControl(type = "none")
tmp <- trainControl(type = "none")
# Random Forest
RFfit <- train(classe ~ ., data = insTrain, trControl = tmp, method = "rf")
tmp
tmp <- trainControl(type = "none")
RFfit <- train(classe ~ ., data = insTrain, method = "rf")
library(randomForest)
RFfit <- randomForest(classe ~ ., data = insTrain)
?randomForest
RFfit <- randomForest(classe ~ ., data = insTrain, do.trace = T)
confusionMatrix(outTrain$classe, predict(RFfit, outTrain))
library(gbm)
?gbm
GBfit <- gbm(classe ~ ., data = insTrain)
# Gradient Boosting
confusionMatrix(outTrain$classe, predict(GBfit, outTrain))
confusionMatrix(outTrain$classe, predict(GBfit, outTrain))
GBfit
predict(GBfit, outTrain)
?predict.gbm
predict(GBfit, outTrain, 50)
predict(GBfit, outTrain, 1)
predict(GBfit, outTrain, single.tree = T)
predict(GBfit, outTrain, 1)
predict(GBfit, outTrain, 8)
predict(GBfit, outTrain, 10000)
GBfit
predict(GBfit, outTrain, 100)
confusionMatrix(outTrain$classe, predict(GBfit, outTrain, 100))
predict(GBfit)
confusionMatrix(outTrain$classe, predict(GBfit, outTrain, 100, type = "response"))
predict(GBfit, outTrain, 100, type = "response")
predict(GBfit, outTrain, 1, type = "response")
predict(GBfit, outTrain, 1000, type = "response")
a <- predict(GBfit, outTrain, 1000, type = "response")
rowSums(a)
which.max(a)
which.max(a[,])
?adaboost
GLMfit <- glm(classe ~ ., data = insTrain)
GLMfit <- glm(classe ~ ., data = insTrain)
GLMfit <- glm(classe ~ ., data = insTrain)
GLMfit <- glm(classe ~ ., data = insTrain, family = "binomial")
GLMfit
confusionMatrix(outTrain$classe, predict(GLMfit, outTrain, 100))
predict(GLMfit)
?glm
GLMfit <- glm(classe ~ ., data = insTrain, family = "poisson")
CARTfit <- rpart(classe ~ ., data = insTrain)
CARTfit
confusionMatrix(outTrain$classe, predict(CARTfit, outTrain, 100))
# Gradient Boosting
confusionMatrix(outTrain$classe, predict(CARTfit, outTrain))
predict(CARTfit)
predict(CARTfit, outTrain)
?predict.rpart
predict(CARTfit, outTrain, type = "class")
confusionMatrix(outTrain$classe, predict(CARTfit, outTrain, type = "class"))
confusionMatrix(outTrain$classe, predict(RFfit, outTrain))
installl
install.packages("rpart.plot")
library(rpart)
library(rpart.plot)
rpart.plot(CARTfit)
?createFolds
pairs(train)
pairs(insTrain)
?createFolds
a <- createFolds(insTrain, 3)
str(a)
a <- createFolds(insTrain$classe, 3)
a
str(a)
?train
tmp <- trainControl(method = "none")
# Random Forest
RFfit <- train(classe ~ ., data = insTrain, trControl = tmp, method = "rf")
tmp <- trainControl(method = "none")
# Random Forest
RFfit <- train(classe ~ ., data = insTrain, trControl = tmp, method = "rf")
?trainControl
modelLookup("rf")
a <- gbm(classe ~., data = insTrain
)
a
a$fit
a
?predict.gbm
predict(a, 100, type = "class")
predict(a, 10, type = "class")
predict(a)
predict(a, n.trees = 1)
predict(a, n.trees = 1, type = "class")
predict(a, n.trees = 1, type = "response)
predict(a, n.trees = 1, type = "response")
predict(a, n.trees = 1, type = "link")
predict(a, n.trees = 1, type = "response")
predict(a, n.trees = 1, type = "link")
predict(a, n.trees = 1, type = "response")
plot(a)
plot(a, i="rm")
plot(a, i="classe")
install.packages("adaBoost")
install.packages("adaboost")
install.packages("adabag")
library(adabag)
a <- adabag::boosting(classe~.,data = insTrain)
rm(a)
a <- glm(classe~., data = insTrain)
a <- glm(classe~., data = insTrain, family = "gaussian")
a <- glm(classe~., data = insTrain, family = "binomial")
a
predict(a)
?predict.glm
predict(a, type = "response")
predict(a, type = "terms")
a <- glm(classe~., data = insTrain, family = "multinomial")
?glm
?rfcv
RFfit <- randomForest(classe ~ ., data = insTrain, do.trace = T)
RFfit2 <- rfcv(insTrain[,1:52], insTrain[,53], cv.fold = 2)
RFfit2
RFfit2$error.cv
confusionMatrix(outTrain$classe, predict(RFfit2, outTrain))
predict(RFfit2)
confusionMatrix(outTrain$classe, RFfit2))
confusionMatrix(outTrain$classe, RFfit2)
str(RFfit2)
confusionMatrix(outTrain$classe, RFfit2$predicted$`52`)
RFfit2
RFfit2
RFfit2$n.var
RFfit2$error.cv
varImpPlot(RFfit)
confusionMatrix(outTrain$classe, RFfit2$predicted$`52`)
length(RFfit2$predicted$`26`)
length(insTrain$classe)
confusionMatrix(outTrain$classe, RFfit2$predicted$`26`)
confusionMatrix(outTrain$classe, t(RFfit2$predicted$`26`))
RFfit$oob.times
setwd("~/Google Drive/DataScienceClasses/Machine Learning")
library(dplyr)
library(caret)
#library(ggplot2)
library(randomForest)
library(rpart)
library(rpart.plot)
set.seed(4711)
if(!file.exists("train.csv")){
download.file("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv", "train.csv", method = "curl")
}
if(!file.exists("test.csv")){
download.file("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv", "test.csv", method = "curl")
}
train <- read.csv("train.csv", na.strings = c("","NA","#DIV/0!"))
test <- read.csv("test.csv", na.strings = c("","NA","#DIV/0!"))
# This takes care of the NA colums
remove <- apply(train, 2, function(x){mean(is.na(x))})
#levels(factor(removeNA))
remove <- remove > 0.9
# This takes care of the first 7 colums
remove[1:7] = TRUE
# Get rid of colums in test and training data
train <- train[,!remove]
test <- test[,!remove]
insamp <- createDataPartition(train$classe, p = 0.65, list = FALSE)
insTrain <- train[insamp, ]
outTrain <- train[-insamp, ]
# CARTree
CARTfit <- rpart(classe ~ ., data = insTrain)
# Random Forest
RFfit <- randomForest(classe ~ ., data = insTrain, do.trace = F)
varImpPlot(RFfit$finalModel)
varImpPlot(RFfit)
importance(RFfit)
sort(importance(RFfit))
sort(-importance(RFfit))
sort(importance(RFfit))
sort(importance(RFfit), desc)
sort(importance(RFfit))
a <- importance(RFfit)
View(data.frame(a))
confusionMatrix(outTrain$classe, predict(CARTfit, outTrain, type = "class"))
# Random Forest
confusionMatrix(outTrain$classe, predict(RFfit, outTrain))
RFfit_fin <- train(classe ~ ., data = train, trControl = trnCtrl, method = "rf")
RFfit_fin <- randomForest(classe ~ ., data = train)
RFfit_fin
prediction <- predict(RFfit_fin, test)
prediction
print(data.frame(test$problem_id,prediction))
print(data.frame(id = test$problem_id, prediction))
---
title: "Machine Learning Course Project"
author: "CA"
date: "February 28, 2016"
output: html_document
---
# Executive Summary
```{r}
setwd("~/Google Drive/DataScienceClasses/Machine Learning")
library(dplyr)
library(caret)
#library(ggplot2)
library(randomForest)
library(rpart)
library(rpart.plot)
set.seed(4711)
```
# Getting and Cleaning Data
Downloading the data:
```{r}
if(!file.exists("train.csv")){
download.file("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv", "train.csv", method = "curl")
}
if(!file.exists("test.csv")){
download.file("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv", "test.csv", method = "curl")
}
train <- read.csv("train.csv", na.strings = c("","NA","#DIV/0!"))
test <- read.csv("test.csv", na.strings = c("","NA","#DIV/0!"))
```
Looking at the train data, there are several columns that are mostly or only NAs, so essentially we cannot use them for prediction, hence we can remove them in both, training and test data. A cutoff was chosen to remove
Also the first couple of columns have data that might skew the prediction, like the timestamp or the username (i.e. certain excercises were only done at a certain time and/or by a certain user - while usually a valid classifier, the goal here is to identify the exercise purely based on the sensor readings rather than other patterns)
```{r}
# This takes care of the NA colums
remove <- apply(train, 2, function(x){mean(is.na(x))})
#levels(factor(removeNA))
remove <- remove > 0.9
# This takes care of the first 7 colums
remove[1:7] = TRUE
# Get rid of colums in test and training data
train <- train[,!remove]
test <- test[,!remove]
```
# Splitting the Data
Since we want to make sure to have a good classifier before we predict the test-set, we will split the training set into an in-sample and out-sample set at a 65/35 ratio.
```{r}
insamp <- createDataPartition(train$classe, p = 0.65, list = FALSE)
insTrain <- train[insamp, ]
outTrain <- train[-insamp, ]
```
# Model Testing
In order to find the best prediction model, we will try a CARTree fit first, and then Random Forest for the classification. I prefer to use the function from the packages directly rather than the caret-wrapper.
```{r}
# CARTree
CARTfit <- rpart(classe ~ ., data = insTrain)
# Random Forest
RFfit <- randomForest(classe ~ ., data = insTrain, do.trace = F)
varImpPlot(RFfit)
```
## Validation
```{r}
# CARTree
confusionMatrix(outTrain$classe, predict(CARTfit, outTrain, type = "class"))
# Random Forest
confusionMatrix(outTrain$classe, predict(RFfit, outTrain))
```
# Model re-training for greatest accuracy
In order to make even better predictions for the test set, the final model will incorporate the entire training data set. Hence, we have to re-train our model:
```{r}
RFfit_fin <- randomForest(classe ~ ., data = train, do.trace = F)
```
# Predicting on the test set for submission
```{r}
prediction <- predict(RFfit_fin, test)
print(data.frame(id = test$problem_id, prediction))
```
rpart.plot(CARTfit)
confusionMatrix(outTrain$classe, predict(CARTfit, outTrain, type = "class"))
confusionMatrix(outTrain$classe, predict(RFfit, outTrain))
prediction <- predict(RFfit_fin, test)
print(data.frame(id = test$problem_id, prediction))
q()
